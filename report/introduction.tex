\section{Introduction}

Nowadays, web applications store and process huge amount of data, which requires high scalability of the underlying distributed file system. Although Google File System~\cite{gfs}, Hadoop Filesystem~\cite{hdfs}, etc are proposed to efficiently store the huge amount of data, they all assume the metadata of the files are compact enought to be stored on a single machine. Unlike data, the modification of metadata imposes strong transaction processing guarantee, and it is far easier to ensure ACID on single machine than on distributed set of machines.

However, under a lot of web application scenarios, small files are generated frequently where a single machine is not enough to store all the metadata. In other word, the metadata storage of the distributed filesystem becomes the bottleneck of a truly scalable storage layer. CalvinFS is a highly scalable distributed filesystem, which stores files metadata in a distributed database rather than in a single server. This design choice dramatically increases the scalability of the filesystem, and as well overcomes single-point failure of metadata server so that provides higher availability. However, such optimization incurs additional overhead on manipulating file metadata. Originally, modifying the metadata of a file only needs a single RPC call from the client to the metadata server, while now it needs a distributed transaction to achieve this goal. By exploiting the idea from Calvin paper, CalvinFS could achieve high throughput for distributed transactions by reducing the lock holding period.

Moreover, in order to fulfill stringent availability requirement, data are often replicated in geographically separated regions, to overcome the entire datacenter outage caused by natural disaster. In such scenario, the cross-region distributed transaction will result in unacceptably high latency. To be specific, in CalvinFS, the latency mainly stems from following two aspects: 1) the Paxos-based protocol for global ordering of all distributed transactions to maintain consistency between replicas; 2) Concurrency control when executing transactions, such as Two-phase Locking (2PL) to guarantee the isolation property.

In this paper, we propose \name{} system to mitigate performance drop under geo-distributed setting. We first use hash-based method to assign different paths to different master replica, so that transactions within a single master replica could avoid global cross-replica ordering. Then, targeting at multi-replica transaction, we design the remaster protocol to adaptively adjust the mapping between the file path and its master replica, so as to adapt all these transactions to single-replica transactions. This report is organized as follow: Section 2 briefly introduces the background and motivation for our system; Section 3 illustrates the system architecture and the remaster protocol.; Section 4 further explains how the protocol works under complicated scenario; Section 5 introduces the details of our implementation; and Section 6 will introduces the ongoing works and future works.

